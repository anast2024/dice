/*
 * Copyright (C) Huawei Technologies Co., Ltd. 2023-2025. All rights reserved.
 * SPDX-License-Identifier: MIT
 */
/*******************************************************************************
 * @file tsano.c
 * @brief Implements an empty TSAN interface.
 *
 * This file implements an empty TSAN interface. Atomic operations behave as
 * expected compiler builtins and seq_cst barriers.
 ******************************************************************************/
#include <stdint.h>
#define _tmpl_mute /************ simple empty macros to make clangd happy. ****/
#define _tmpl_map(...)
#define _tmpl_begin(...)
#define _tmpl_end(...)
#define __atomic_fetch_OP __atomic_exchange_n
#define uintBITS_t int
#define PARAM_SUF  0
#define _tmpl_unmute /*********************************************************/

/* empty tsan initialization */
void __tsan_init() {}
void __tsan_write_range(void) {}
void __tsan_read_range(void) {}

/* empty vptr impl */
void __tsan_vptr_read(void **vptr_p) {}
void __tsan_vptr_update(void **vptr_p, void *new_val) {}

/* with GCC version < 10, this symbols is defined */
void internal_sigreturn(void) {}

void __tsan_mutex_pre_lock(void *addr, unsigned flags) {}
void __tsan_mutex_post_lock(void *addr, unsigned flags, int recursion) {}
int __tsan_mutex_pre_unlock(void *addr, unsigned flags) { return 0; }
void __tsan_mutex_post_unlock(void *addr, unsigned flags) {}
void __tsan_mutex_create(void *addr, unsigned flags) {}
void __tsan_mutex_destroy(void *addr, unsigned flags) {}
void __tsan_acquire(void *addr) {}
void __tsan_release(void *addr) {}
void __tsan_func_entry(void *pc) { }
void __tsan_func_exit(void)  { }

/* plain reads and writes */
_tmpl_map(nix,)
_tmpl_begin(PFX=[[nix;unaligned_]], FUNC=[[read;write]], SZ=[[1;2;4;8;16]])
void __tsan_PFXFUNCSZ(void *a) { }
_tmpl_end()

/* plain reads and writes 2 values */
_tmpl_begin(FUNC=[[read;write]], SZ=[[1;2;4;8;16]])
void __tsan_FUNCSZ_pc(void *a, void *b) { }
_tmpl_end()

/* atomic loads */
_tmpl_begin(BITS=[[8;16;32;64]])
uintBITS_t __tsan_atomicBITS_load(const volatile uintBITS_t *a, int mo) {
    return __atomic_load_n(a, __ATOMIC_SEQ_CST);
}
_tmpl_end()

/* atomic stores */
_tmpl_begin(BITS=[[8;16;32;64]])
void __tsan_atomicBITS_store(volatile uintBITS_t *a, uintBITS_t v, int mo) {
    __atomic_store_n(a, v, __ATOMIC_SEQ_CST);
}
_tmpl_end()


/* xchg */
_tmpl_begin(BITS=[[8;16;32;64]])
uintBITS_t  __tsan_atomicBITS_exchange(volatile uintBITS_t *a, uintBITS_t v, int mo) {
    return __atomic_exchange_n(a, v, __ATOMIC_SEQ_CST);
}
_tmpl_end()

/* fetch_RMW */
_tmpl_begin(OP=[[add;sub;and;or;xor;nand]], BITS=[[8;16;32;64]])
uintBITS_t  __tsan_atomicBITS_fetch_OP(volatile uintBITS_t *a, uintBITS_t v, int mo) {
    return __atomic_fetch_OP(a, v, __ATOMIC_SEQ_CST);
}
_tmpl_end()


/* compare_exchange_{strong,weak} */
_tmpl_map(PARAM_strong, 0)
_tmpl_map(PARAM_weak, 1)
_tmpl_begin(SUF=[[strong;weak]], BITS=[[8;16;32;64]])
int __tsan_atomicBITS_compare_exchange_SUF(
        volatile uintBITS_t *a, uintBITS_t *c, uintBITS_t v, int mo) {
    return __atomic_compare_exchange_n(a, c, v, PARAM_SUF, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);
}
_tmpl_end()

/* compare_exchange_val */
_tmpl_begin(BITS=[[8;16;32;64]])
uintBITS_t __tsan_atomicBITS_compare_exchange_val(volatile uintBITS_t *a, uintBITS_t c, uintBITS_t v, int mo) {
    (void)__atomic_compare_exchange_n(a, &c, v, 0, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);
    return c;
}
_tmpl_end()

/* atomic fences */
void __tsan_atomic_thread_fence(int  mo)
{
    (void) mo;
    __atomic_thread_fence(__ATOMIC_SEQ_CST);
}

void __tsan_atomic_signal_fence(int mo)
{
    (void) mo;
   __atomic_signal_fence(__ATOMIC_SEQ_CST);
}
